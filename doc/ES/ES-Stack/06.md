```
+++
author = "kmplex"
title = "ELK Stack 개발부터 운영까지 5장"
date = "2022-03-23"
description = "Elastic Search 6장"
series = ["ES"]
categories = ["dev"]
+++
```

### Logstash

- 로그는 반정형 데이터이기에, 데이터 분석을 하기 위해선 수집 / 가공 / 전송하는 일련의 과정이 필요하다.
    - Logstash 가 이를 지원한다.

### Logstash 소개

- Logstash 는 플러그인 기반의 오픈소스 데이터 처리 파이프라인 도구.
    - 데이터 전처리 과정을 별도 App 작성 없이, 설정만으로 수행할 수 있다.

### 특징

- 플러그인 기반
    - pipeline 을 구성하는 각 요소들은 전부 plugin 형태로 만들어져있다.
    - 필요 시, 간단한 코드로 전용 플러그인을 작성하여, 고유 로직을 포함 시킬 수 있다.
- 모든 형태의 데이터 처리
    - 기본 제공되는 플러그인들의 조합만으로, 다양한 형태의 데이터를 입력받아 가공한 다음 저장 할 수 있다.
    - 특히, 이벤트 데이터를 처리하는데 최적화 되어있다.
- 성능
    - 내장되어 있는 메모리와 파일 기반 큐를 사용하므로, 처리 속도와 안정성이 높다.
    - 또한 벌크 인덱싱 혹은 배치 크기 조정을 통해 병목 현상을 방지하고 성능을 최적화 할 수 있다.
- 안정성
    - 재시도 로직이나, 오류가 발생한 도큐먼트를 따로 보관하는 dead letter queue 를 내장하고 있다.
        - 장애 상황에서 도큐먼트 유실을 최소화 할 수 있다.

### 실행

```text
# 실행 
 .\logstash.bat -e "input { stdin {} } output { stdout {} }"
# 입력
hello world 

# 출력
# json 형태로 데이터를 출력하는데, version / timestamp 는 사용자 컬럼과 충돌이 날 수 있어, @ 를 붙여두었다. 
{
      "@version" => "1",
       "message" => "hello world\r",
    "@timestamp" => 2022-03-23T09:55:44.405340200Z,
         "event" => {
        "original" => "hello world\r"
    },
          "host" => {
        "hostname" => "LAPTOP-K90L8S2L"
    }
}
```

### 파이프라인

- Logstash 의 가장 중요한 부분으로, 데이터를 입력 받아, 실시간으로 변경하여 다른 시스템에 전달하는 역할을 한다.
    - 입력(필수) / 필터 (옵션) / 출력 (필수) 이라는 세 가지 구성요소로 이루어진다.
    - 파이프라인에 입력 / 출력은 반드시 포함되어 있어야하며, 각 단계에서 복수개의 플러그인을 포함 할 수 있다.
- 기본적인 파이프라인 템플릿은 아래와 같다.

```text
input { 
  { 입력 플러그인 }
}

filter {
  { 필터 플러그인}
}

output {
  { 출력 플러그인 }
}
```

### 입력

- 파이프라인의 가장 앞부분에 위치하여, 데이터를 입력받는 단계이다.
    - 직접 접근하여 읽을 수 있으나, 서버를 열어놓고 받아들이는 형태로도 구성이 가능하다.
    - 파일 / 통계 / 웹 / DB / Stream 등으로 데이터를 읽을 수 있다.
- 자주 사용하는 입력 플러그인은 아래와 같다
    - file => tail -f 명령어 처럼 파일을 스트리밍하며 이벤트를 읽어 들인다.
    - syslog => 네트워크를 통해 전달되는 syslog를 수신한다.
    - kafka => kafka topic 에서 데이터를 읽어들인다.
    - jdbc => 지정된 시간마다 쿼리를 실행하여, 결과를 읽어 들인다.

```text
# logstash-test.conf 
input {
    file {
        path => "filePath"
        start_position => "beginning"
    }
}

output {
    stdout {}
}

# 실행
.\bin\logstash.bat -f .\config\logstash-test.conf

# message 필드안에 입력된 데이터가 출력 된다.
```

### 필터

- 입력받은 데이터를 의미있는 데이터로 구조화하는 역할을 한다.
    - 필수는 아니나, 필터가 없으면 기능을 온전히 발휘하기 어렵다.
- 비정형 데이터를 정형화하고, 데이터 분석을 위한 구조를 잡아준다.
- 자주 사용하는 필터 플러그인은 아래와 같다.
    - grok => grok 퍁너을 사용하여 메시지를 구조화된 형태로 분석한다.
    - dissect => 간단한 패턴을 사용하여, 메시지를 구조화된 형태로 분석한다.
    - mutate => 필드명을 변경하거나, 문자열 처리등 일반적인 가공 함수들을 제공한다.
    - date => 문자열을 지정한 패턴의 날짜형으로 분석한다.

#### 문자열 자르기

```text
filter {
  mutate {
    # 구분 문자를 기준으로 문자열을 배열로 나눈다.
    split => { "message" = " "}
    # 필드를 새로 만들어 추가한다. 
    add_field => { "id" => "%{[message][2]}" }
  }
}
```

- mutate 의 경우, 옵션이 많아 실행 순서가 존재한다.

#### dissect 를 이용한 문자열 파싱

```text
filter {
  dissect {
    # 패턴을 이용해 문자열을 분석하고 주요 정보를 필드로 추출하는 기능 
    mapping => { "message" => "[%{timestamp}] [%{id}] ${ip} %{port} [%{level}] - %{message}." }
  }
}
```

- mapping option 에 형태를 정의하고 필드를 구분한다.
    - %{필드명} 으로 작성 시, 새로운 필드가 만들어진다.
    - 주의할 점으로, 필드 사이의 구분자가 정해놓은 문자가 아니면, 오류가 발생한다.
- %{?->}
    - ? 을 입력하면, 해당 필드는 결과에 포함 시키지 않는다.
    - `->` 사용 시, 공백이 몇칸이든, 하나로 인식한다.
- %{+필드명}
    - 여러 개의 필드를 하나의 필드로 합쳐서 표현한다.

#### grok 을 이용한 문자열 파싱

- 정규 표현식을 이용해 문자열을 파싱 할 수 있다.

```text
filter {
  grok {
    # 패턴을 이용해 문자열을 분석하고 주요 정보를 필드로 추출하는 기능 
    match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] [ ]*\[%{DATA:id}] %{IP:ip} ${NUMBER:port:int} \[%{LOGLEVEL:level}\] \- %{DATA:msg}\." }
  }
}
```

- 기본적으로 %{패턴명:변수명} 형태로 작성한다.
    - 특정 값을 넣지 않으면, 모든 데이터가 문자 타입으로 인식된다.

#### dissect / grok 주의 점

- dissect / grok 모두 패턴을 통해 구문 분석을 한다는 공통점이 있지만, 성능 차이가 존재한다.
    - 로그 형식이 일정 하고, 패턴이 변하지 않는다면, dissect 를 사용하는 것이 좋다.
    - 만약 로그 형태가 일정하다고 장담하기 어렵다면, grok 을 사용하는 것이 좋다.

#### 대소문자 변경

```text
filter {
  dissect {
    # level 만 빼고 다 제외 시킨다. 
    mapping => { "message" => "[%{?timestamp}] [%{?id}] ${?ip} %{?port} [%{level}] - %{?message}." }
  }
  
  mutate {
    # 대문자로 변경 
    uppercase => ["level"]
  }
}
```

#### 날짜/시간 문자열 분석

- 인입되는 로그의 날짜들이 통일화되어 있지 않다면, date plugin 을 사용 할 수 있다.

```text
filter {
  dissect {
    mapping => { "message" => "[%{timestamp}] [%{?id}] ${?ip} %{?port} [%{?level}] - %{?message}." }
  }
  
  mutate {
    # 앞 뒤 공백을 제거한다.
    strip => timestamp
  }
  
  date {
    # timestamp 필드에 대하여, 날짜/ 시간 포맷이 일치하면, ISO8601 타입으로 변경한다.
    match => [ "timestamp", "YYYY-MM-dd HH:mm", "yyyy/MM/dd HH:mm:ss" ]
    # 저장될 필드 
    target => "new_timestamp"
    # 타임존 정보가 포함되어 있지 않다면, UTC 로 설정한다.
    timezone => "UTC"
  }
}
```
#### 조건문

- 입력되는 이벤트의 형태가 다양하기에, 각기 다른 필터를 적용해야하는 경우가 있다.

```text
filter {
  dissect {
    mapping => { "message" => "[%{timestamp}] [%{id}] ${ip} %{port} [%{level}] - %{message}." }
  }
  if [level] == "INFO" {
    # 데이터를 삭제하는 플러그 인
    drop { }
  }
  else if [level] == "warn" {
    mutate {
      remove_field => [ "ip", "port", "timestamp", "level" ]
    }
  }
}
```

- 개인적인 생각으론, 프로그래밍이 들어간 순간부터, application 단으로 로직이 이동되어야하지 않을까? 하는 생각이 듬

### 출력 

- 파이프라인의 마지막 단계로, 가공된 데이터를 지정한 대상으로 내보내는 단계
- 자주 사용되는 출력 플러그인은 아래와 같다.
  - elasticsearch => bulk API를 사용해 ES에 indexing 을 수행한다.
  - file => 지정한 파일의 새로운 줄에 데이터를 기록한다.
  - kafka => 카프카 토픽에 데이터를 기록한다.

```text
output {
  file {
  # 파일 플러그인 으로 데이터를 파일 형식으로 전송한다.
    path => "filePath"
  }
  # ES 로 데이터를 전송한다.
  elasticsearch {
    index => "output"
  }
}
```

### 코덱 

- 코덱은 입력 / 출력 / 필터와 달리 독립적으로 동작하지 않고, 입력 / 출력 과정에서 사용되는 플러그인 이다.
  - 입/출력 시, 메시지를 적절한 형태로 변환하는 스트림 필터이다.
  - 입/출력 단계에서 인코딩 / 디코딩을 담당한다.

```text
input {
    file {
        path => "filePath"
        start_position => "beginning"
        # json 형태로 메시지를 읽어들인다.
        codec => "json" # plain 
        # 출력 시, debug 모드인, rubydebug 코덱도 지원한다.
    }
}
```

### 다중 파이프라인

- 하나의 logstash 에서 여러개의 파이프라인을 동작시킬 수 있다.
  - 각 파이프 라인당 logstash 를 띄우는건 모니터링이 어렵고, 효율적인 방식이 아니다.
  - 또한 분기문을 사용하는 방식도 유지보수가 어려워진다.

#### 다중 파이프라인 작성 

- `pipelines.yml` 파일을 수정해야한다.
  - pipeline.id => pipeline 의 고유한 아이디
  - path.config => 설정 파일의 위치
  - pipeline.workers => 필터 / 출력을 병렬로 처리하기 위한 워커 수 (CPU 코어 수와 동일)
  - pipeline.batch.size => 입력 시, 하나의 워커당 최대 몇 개까지의 이벤트를 동시에 처리할지 결정한다.
    - 배치 처리된 이벤트들은 엘라스틱서치 출력에서 하나의 벌크 요청으로 묶이기에, 수치가 클 수록 요청 수행 횟수가 준다.
    - 다만, 단일 요청이 커지므로, 튜닝할 필요가 있다.
  - queue.type => pipeline 에서 사용 할 queue 종류를 정할 수 있다.
  

### 모니터링 

- API 를 활용하여 모니터링이 가능하다.
  - curl "localhost:9600/_node?pretty" #노드
  - curl "localhost:9600/_node/plugins?pretty" #플러그인
  - curl "localhost:9600/_node/stats?pretty" #노드 통계
  - curl "localhost:9600/_node/hot_threads?pretty" #핫 스레드
- 모니터링 기능 활성화 시, ES를 통한 모니터링이 가능하다.